import time
import shutil
from pathlib import Path
from rag_core import RagEngine

# Configuraci√≥n de rutas
# Ajustamos para que apunte correctamente a donde Docker monta los vol√∫menes
BASE_DIR = Path(__file__).parent.parent 
MODEL_PATH = BASE_DIR / "data" / "models" / "all-MiniLM-L6-v2"
RERANK_PATH = BASE_DIR / "data" / "models" / "ms-marco-TinyBERT-L-2-v2" # Nueva ruta
DB_PATH = BASE_DIR / "data" / "databases" / "chroma_benchmark"

# Texto de prueba (Lorem Ipsum largo o texto real)
SAMPLE_TEXT = """
La arquitectura hexagonal, o patr√≥n de puertos y adaptadores, es un patr√≥n arquitect√≥nico 
utilizado en el dise√±o de software. Su objetivo es crear componentes de aplicaci√≥n 
d√©bilmente acoplados que puedan conectarse f√°cilmente a su entorno de software 
mediante puertos y adaptadores. Esto hace que los componentes sean intercambiables 
en cualquier nivel y facilita la automatizaci√≥n de las pruebas.

ConfidentialAI es una aplicaci√≥n dise√±ada para ejecutarse localmente sin depender de la nube.
Utiliza tecnolog√≠as como ONNX para inferencia ligera y ChromaDB para almacenamiento vectorial.
El objetivo es mantener la privacidad de los datos del usuario.

PyInstaller es una herramienta que congela aplicaciones Python en ejecutables independientes.
Para optimizar el tama√±o, es crucial evitar librer√≠as pesadas como PyTorch o TensorFlow 
si solo se necesita inferencia.
"""  # Multiplicamos para tener volumen

def run_benchmark():
    # 1. Limpieza inicial (para que el test sea justo)
    if DB_PATH.exists():
        print(f"üßπ Limpiando base de datos previa en {DB_PATH}...")
        shutil.rmtree(DB_PATH)
    
    print("="*60)
    print("üöÄ INICIANDO BENCHMARK DE RAG H√çBRIDO (ONNX + CHROMA + BM25)")
    print("="*60)

    # 2. Medir tiempo de carga (Cold Start)
    start_time = time.perf_counter()
    # Inicializamos el motor h√≠brido
    engine = RagEngine(str(MODEL_PATH), str(RERANK_PATH), str(DB_PATH))
    load_time = time.perf_counter() - start_time
    print(f"‚è±Ô∏è  Carga de Modelos (Cold Start): {load_time:.4f} segundos")

    # 3. Medir tiempo de Ingesta (Embedding + Indexado + BM25)
    print("\nüì• Ingestando documentos...")
    start_time = time.perf_counter()
    # La nueva firma de ingest acepta project_id, usamos el default
    num_chunks = engine.ingest(SAMPLE_TEXT)
    ingest_time = time.perf_counter() - start_time
    print(f"‚è±Ô∏è  Ingesta ({len(SAMPLE_TEXT)} caracteres, {num_chunks} chunks hijos): {ingest_time:.4f} segundos")
    print(f"üìä Velocidad de Ingesta: {len(SAMPLE_TEXT)/ingest_time:.2f} chars/seg")

    # 4. Medir tiempo de Consulta (Retrieval)
    query = "¬øQu√© es ConfidentialAI y qu√© tecnolog√≠as usa?"
    print(f"\nüîç Query: '{query}'")
    
    start_time = time.perf_counter()
    
    # CORRECCI√ìN AQU√ç: 
    # 1. Usamos .search() en lugar de .query()
    # 2. El resultado ahora es un string (contexto completo), no un DataFrame
    context_result = engine.search(query, top_k=3)
    
    query_time = time.perf_counter() - start_time
    
    print(f"‚è±Ô∏è  Tiempo de B√∫squeda H√≠brida: {query_time:.4f} segundos")
    
    print("\n--- Contexto Recuperado (Documentos Padre) ---")
    print(context_result)
    print("-" * 60)

if __name__ == "__main__":
    print("="*60)
    if not MODEL_PATH.exists():
        print(f"‚ùå Error: No se encuentra el modelo en {MODEL_PATH}")
        print("   Aseg√∫rate de que los vol√∫menes de Docker est√©n bien montados o ejecuta setup_models.py")
    else:
        print(f"‚úÖ Modelo encontrado en {MODEL_PATH}")
        run_benchmark()